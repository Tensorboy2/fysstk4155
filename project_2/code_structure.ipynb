{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Class structure:\n",
    "\n",
    "src\n",
    "├── model\n",
    "│   ├── Loss functions:\n",
    "│   │   ├── **Mean squared error (MSE)**: Used for regression tasks to minimize the squared difference between predicted and actual values.\n",
    "│   │   └── **Cross entropy**: Classification tasks to measure the difference between predicted probabilities and actual classes.\n",
    "│   ├── NN (Neural Network):\n",
    "│   │   ├── **Layers**: \n",
    "│   │   │   ├── Input layer: Takes the shape of the input data.\n",
    "│   │   │   ├── Hidden layers: Configurable number of layers and neurons per layer (e.g., `self.hidden_layers`, `self.hidden_sizes`).\n",
    "│   │   │   └── Output layer: Configurable based on the task (regression or classification).\n",
    "│   │   ├── **Activation functions**: \n",
    "│   │   │   ├── ReLU, Sigmoid, Tanh: Common hidden layer activations.\n",
    "│   │   │   └── Optional activation for output layer (e.g., softmax for classification).\n",
    "│   │   ├── **Weight and bias storage**: Weights and biases are stored in `self.params` as a dictionary (or other structure).\n",
    "│   │   └── **Forward propagation**: Implements forward pass through the layers, applying activations.\n",
    "│   └── Regression:\n",
    "│       ├── **Design matrix generation**: Creates design matrix `X` based on data shape and polynomial fit degree for tasks like linear or polynomial regression.\n",
    "│       └── **Parameter storage**: Stores regression coefficients in `self.params` (e.g., after using OLS or Ridge).\n",
    "├── optimizer\n",
    "│   ├── **Base optimizer**:\n",
    "│   │   ├── Common attributes (e.g., learning rate).\n",
    "│   │   ├── **Step function**: Interface that each optimizer must implement to update weights.\n",
    "│   └── **Subclasses**:\n",
    "│       ├── **Gradient Descent (GD)**: Basic gradient descent algorithm.\n",
    "│       ├── **Stochastic Gradient Descent (SGD)**: Uses mini-batches for updating weights.\n",
    "│       ├── **AdaGrad (With GD and SGD)**: Can use mini-batches for updating weights.\n",
    "│       ├── **RMSprop**: Uses mini-batches for updating weights.\n",
    "│       ├── **Adam**: Momentum-based optimizer.\n",
    "│       └── Each class stores the momentum and other parameters as its state.\n",
    "├── train\n",
    "│   └── **Train function**:\n",
    "│       ├── **Training loop**: \n",
    "│       │   ├── Loops through the dataset over a number of epochs.\n",
    "│       │   ├── At each epoch, it calls `optimizer.step()` to update the weights.\n",
    "│       │   └── Evaluates the model on validation data during training.\n",
    "│       ├── **Loss tracking**: Stores loss over epochs to monitor convergence.\n",
    "│       └── **Early stopping**: Optional feature to stop training when performance plateaus.\n",
    "└── utils\n",
    "    ├── Evaluation functions:\n",
    "    │   ├── **Mean Squared Error (MSE)**: Measures the average squared difference between predicted and actual values (for regression).\n",
    "    │   └── **R2 score**: Measures the proportion of variance explained by the model (for regression).\n",
    "    └── Plotting:\n",
    "        ├── **Accuracy plots**: Plots accuracy or performance as a function of hyperparameters (learning rate, batch size, etc.).\n",
    "        ├── **Loss over epochs**: Plots loss during training to visualize convergence.\n",
    "        └── **Classification coefficients**: For linear models, visualize the learned coefficients.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
